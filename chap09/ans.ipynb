{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 80. ID番号への変換"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "問題51で構築した学習データ中の単語にユニークなID番号を付与したい．学習データ中で最も頻出する単語に1，2番目に頻出する単語に2，……といった方法で，学習データ中で2回以上出現する単語にID番号を付与せよ．そして，与えられた単語列に対して，ID番号の列を返す関数を実装せよ．ただし，出現頻度が2回未満の単語のID番号はすべて0とせよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 81. RNNによる予測"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ID番号で表現された単語列$ x=(x_1,x_2,…,x_T) $がある．ただし，$ T $は単語列の長さ，$ x_t \\in \\mathbb{ R }^V $は単語のID番号のone-hot表記である（$ V $は単語の総数である）．再帰型ニューラルネットワーク（RNN: Recurrent Neural Network）を用い，単語列$ x $からカテゴリ$ y $を予測するモデルとして，次式を実装せよ．\n",
    "$$\n",
    "    \\vec h_0=0, \\\\\n",
    "    \\vec h_t=\\overrightarrow{ RNN }(emb(x_t),\\vec h_{t−1}), \\\\\n",
    "    y=softmax(W^{(yh)}\\vec h_T+b^{(y)})\n",
    "$$\n",
    "ただし，$ emb(x) \\in \\mathbb{ R }^{d_w} $は単語埋め込み（単語のone-hot表記から単語ベクトルに変換する関数），$ \\vec h_t \\in \\mathbb{ R }^{dh} $は時刻$ t $の隠れ状態ベクトル，$ \\overrightarrow{ RNN }(x,h) $は入力$ x $と前時刻の隠れ状態$ h $から次状態を計算するRNNユニット，$ W^{(yh)} \\in \\mathbb{ R }^{L\\times d_h} $は隠れ状態ベクトルからカテゴリを予測するための行列，$ b^{(y)} \\in \\mathbb{ R }^L $はバイアス項である（$ dw,dh,L $はそれぞれ，単語埋め込みの次元数，隠れ状態ベクトルの次元数，ラベル数である）．RNNユニット$ \\overrightarrow{ RNN }(x,h) $には様々な構成が考えられるが，典型例として次式が挙げられる．\n",
    "$$\n",
    "    \\overrightarrow{ RNN }(x,h)=g(W^{(hx)}x+W^{(hh)}h+b^{(h)})\n",
    "$$\n",
    "ただし，$ W^{(hx)} \\in \\mathbb{ R }^{dh\\times dw}，W^{(hh)} \\in \\mathbb{ R }^{dh\\times dh},b^{(h)} \\in \\mathbb{ R }^{dh} $はRNNユニットのパラメータ，$ g $は活性化関数（例えば$ tanh $や $ReLU $など）である．  \n",
    "なお，この問題ではパラメータの学習を行わず，ランダムに初期化されたパラメータで$ y $を計算するだけでよい．次元数などのハイパーパラメータは，$ d_w=300,d_h=50 $など，適当な値に設定せよ（以降の問題でも同様である）．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 82. 確率的勾配降下法による学習Permalink"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "確率的勾配降下法（SGD: Stochastic Gradient Descent）を用いて，問題81で構築したモデルを学習せよ．訓練データ上の損失と正解率，評価データ上の損失と正解率を表示しながらモデルを学習し，適当な基準（例えば10エポックなど）で終了させよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 83. ミニバッチ化・GPU上での学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "問題82のコードを改変し，$ B $事例ごとに損失・勾配を計算して学習を行えるようにせよ（$ B $の値は適当に選べ）．また，GPU上で学習を実行せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 84. 単語ベクトルの導入"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "事前学習済みの単語ベクトル（例えば，Google Newsデータセット（約1,000億単語）での[学習済み単語ベクトル](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing)）で単語埋め込み$ emb(x) $を初期化し，学習せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 85. 双方向RNN・多層化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "順方向と逆方向のRNNの両方を用いて入力テキストをエンコードし，モデルを学習せよ．\n",
    "$$\n",
    "    \\overleftarrow{ h }_{T+1}=0, \\\\\n",
    "    \\overleftarrow{ h }_t=\\overleftarrow{ RNN }(emb(x_t),h_{t+1}), \\\\\n",
    "    y=softmax(W^{(yh)}[\\overrightarrow{ h }_T;\\overleftarrow{ h }_1]+b^{(y)})\n",
    "$$\n",
    "ただし，$ \\overrightarrow{ h }_t \\in \\mathbb{ R }^{d_h},\\overleftarrow{ h }_t \\in \\mathbb{ R }^{d_h} $はそれぞれ，順方向および逆方向のRNNで求めた時刻$ t $の隠れ状態ベクトル，$ \\overleftarrow{ RNN }(x,h) $は入力$ x $と次時刻の隠れ状態$ h $から前状態を計算するRNNユニット，$ W^{(yh)} \\in \\mathbb{ R }^{L\\times 2d_h} $は隠れ状態ベクトルからカテゴリを予測するための行列，$ b^{(y)} \\in \\mathbb{ R }^L $はバイアス項である．また，$ [a;b] $はベクトル$ a $と$ b $の連結を表す。  \n",
    "さらに，双方向RNNを多層化して実験せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 86. 畳み込みニューラルネットワーク (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ID番号で表現された単語列$ x=(x_1,x_2,…,x_T) $がある．ただし，$ T $は単語列の長さ，$ x_t \\in \\mathbb{ R }^V $は単語のID番号のone-hot表記である（$ V $は単語の総数である）．畳み込みニューラルネットワーク（CNN: Convolutional Neural Network）を用い，単語列$ x $からカテゴリ$ y $を予測するモデルを実装せよ．  \n",
    "ただし，畳み込みニューラルネットワークの構成は以下の通りとする．\n",
    "- 単語埋め込みの次元数: $ d_w $\n",
    "- 畳み込みのフィルターのサイズ: 3 トークン\n",
    "- 畳み込みのストライド: 1 トークン\n",
    "- 畳み込みのパディング: あり\n",
    "- 畳み込み演算後の各時刻のベクトルの次元数: $ d_h $\n",
    "- 畳み込み演算後に最大値プーリング（max pooling）を適用し，入力文を$ d_h $次元の隠れベクトルで表現\n",
    "\n",
    "すなわち，時刻$ t $の特徴ベクトル$ p_t \\in \\mathbb{ R }^{d_h} $は次式で表される．\n",
    "$$\n",
    "    p_t=g(W^{(px)}[emb(x_{t−1});emb(x_t);emb(x_{t+1})]+b^{(p)})\n",
    "$$\n",
    "ただし，$ W^{(px)} \\in \\mathbb{ R }^{d_h\\times 3d_w},b^{(p)} \\in \\mathbb{ R }^{d_h} $はCNNのパラメータ，$ g $は活性化関数（例えば$ tanh $や$ ReLU $など），$ [a;b;c] $はベクトル$ a,b,c $の連結である．なお，行列$ W^{(px)} $の列数が$ 3d_w $になるのは，3個のトークンの単語埋め込みを連結したものに対して，線形変換を行うためである．  \n",
    "最大値プーリングでは，特徴ベクトルの次元毎に全時刻における最大値を取り，入力文書の特徴ベクトル$ c \\in \\mathbb{ R }^{d_h}を求める．$ c[i] $でベクトル$ c $の$ i $番目の次元の値を表すことにすると，最大値プーリングは次式で表される．\n",
    "$$\n",
    "    c[i]=\\max_{1\\leq t\\leq T} p_t[i]\n",
    "$$\n",
    "最後に，入力文書の特徴ベクトル$ c $に行列$ W^{(yc)} \\in \\mathbb{ R }^{L\\times d_h} $とバイアス項$ b^{(y)} \\in \\mathbb{ R }^L $による線形変換とソフトマックス関数を適用し，カテゴリ$ y $を予測する．\n",
    "$$\n",
    "    y=softmax(W^{(yc)}c+b^{(y)})\n",
    "$$\n",
    "なお，この問題ではモデルの学習を行わず，ランダムに初期化された重み行列で$ y $を計算するだけでよい．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 87. 確率的勾配降下法によるCNNの学習Permalink"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "確率的勾配降下法（SGD: Stochastic Gradient Descent）を用いて，問題86で構築したモデルを学習せよ．訓練データ上の損失と正解率，評価データ上の損失と正解率を表示しながらモデルを学習し，適当な基準（例えば10エポックなど）で終了させよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 88. パラメータチューニング"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "問題85や問題87のコードを改変し，ニューラルネットワークの形状やハイパーパラメータを調整しながら，高性能なカテゴリ分類器を構築せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 89. 事前学習済み言語モデルからの転移学習Permalink"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "事前学習済み言語モデル（例えば[BERT](https://github.com/google-research/bert)など）を出発点として，ニュース記事見出しをカテゴリに分類するモデルを構築せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
