{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 70. 単語ベクトルの和による特徴量Permalink"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "問題50で構築した学習データ，検証データ，評価データを行列・ベクトルに変換したい．例えば，学習データについて，すべての事例$ x_i $の特徴ベクトル$ x_i $を並べた行列$ \\boldsymbol{X} $と，正解ラベルを並べた行列（ベクトル）$ \\boldsymbol{Y} $を作成したい．\n",
    "$$\n",
    "    \\boldsymbol{X}=\n",
    "        \\left[\\begin{array}{c}\n",
    "            x_1 \\\\\n",
    "            x_2 \\\\\n",
    "            ... \\\\\n",
    "            x_n \\\\\n",
    "        \\end{array}\\right] \\quad\n",
    "    \\in\\mathbb{ R }^{n\\times d}\n",
    "    ,\n",
    "    \\boldsymbol{Y}=\n",
    "        \\left[\\begin{array}{c}\n",
    "            y_1 \\\\\n",
    "            y_2 \\\\\n",
    "            ... \\\\\n",
    "            y_n \\\\\n",
    "        \\end{array}\\right] \\quad\n",
    "    \\in\\mathbb{ N }^n    \n",
    "$$\n",
    "ここで，$ n $は学習データの事例数であり，$ x_i \\in \\mathbb{ R }^d $と$ y_i \\in \\mathbb{ N } $はそれぞれ，$ i \\in \\{1,...,n\\} $番目の事例の特徴量ベクトルと正解ラベルを表す． なお，今回は「ビジネス」「科学技術」「エンターテイメント」「健康」の4カテゴリ分類である．$ \\mathbb{ N } _{<4} $で4未満の自然数（0を含む）を表すことにすれば，任意の事例の正解ラベル$ y_i $は$ y_i \\in \\mathbb{ N }_{<4} $で表現できる． 以降では，ラベルの種類数を$ L $で表す（今回の分類タスクでは$ L=4 $である）．  \n",
    "$ i $番目の事例の特徴ベクトル$ x_i $は，次式で求める．\n",
    "$$\n",
    "    x_i = \\frac{1}{T_i} \\displaystyle \\sum_{t=1}^{T_i} emb(w_i,t)\n",
    "$$\n",
    "ここで，$ i $番目の事例は$ T_i $個の（記事見出しの）単語列$ (w_{i,1},w_{i,2},…,w_{i,T_i}) $から構成され，$ emb(w) \\in \\mathbb{ R }^d $は単語$ w $に対応する単語ベクトル（次元数は$ d $）である．すなわち，$ i $番目の事例の記事見出しを，その見出しに含まれる単語のベクトルの平均で表現したものが$ x_i $である．今回は単語ベクトルとして，問題60でダウンロードしたものを用いればよい．$ 300 $次元の単語ベクトルを用いたので，$ d=300 $である．  \n",
    "$ i $番目の事例のラベル$ y_i $は，次のように定義する．\n",
    "$$\n",
    "    y_i\n",
    "    =\n",
    "    \\begin{cases}\n",
    "        0 (記事x_iが「ビジネス」カテゴリの場合) \\\\\n",
    "        1 (記事x_iが「科学技術」カテゴリの場合) \\\\\n",
    "        2 (記事x_iが「エンターテイメント」カテゴリの場合) \\\\\n",
    "        3 (記事x_iが「健康」カテゴリの場合)\n",
    "    \\end{cases}\n",
    "$$\n",
    "なお，カテゴリ名とラベルの番号が一対一で対応付いていれば，上式の通りの対応付けでなくてもよい．  \n",
    "以上の仕様に基づき，以下の行列・ベクトルを作成し，ファイルに保存せよ．\n",
    "- 学習データの特徴量行列: $ X_{train} \\in \\mathbb{ R }^{N_t\\times d} $\n",
    "- 学習データのラベルベクトル: $ Y_{train} \\in \\mathbb{ N }^{N_t} $\n",
    "- 検証データの特徴量行列: $ X_{valid} \\in \\mathbb{ R }^{N_v\\times d} $\n",
    "- 検証データのラベルベクトル: $ Y_{valid} \\in \\mathbb{ N }^{N_v} $\n",
    "- 評価データの特徴量行列: $ X_{test} \\in \\mathbb{ R }^{N_e\\times d} $\n",
    "- 評価データのラベルベクトル: $ Y_{test} \\in \\mathbb{ R }^{N_e} $\n",
    "\n",
    "なお，$ N_t,N_v,N_e $はそれぞれ，学習データの事例数，検証データの事例数，評価データの事例数である．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "学習データ\n",
      "CATEGORY\n",
      "b    4502\n",
      "e    4223\n",
      "t    1219\n",
      "m     728\n",
      "Name: count, dtype: int64\n",
      "検証データ\n",
      "CATEGORY\n",
      "b    562\n",
      "e    528\n",
      "t    153\n",
      "m     91\n",
      "Name: count, dtype: int64\n",
      "評価データ\n",
      "CATEGORY\n",
      "b    563\n",
      "e    528\n",
      "t    152\n",
      "m     91\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 単語ベクトルのロード\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "file = '../chap07/GoogleNews-vectors-negative300.bin.gz'\n",
    "model = KeyedVectors.load_word2vec_format(file, binary=True)\n",
    "\n",
    "# データのロード\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# ファイル読み込み\n",
    "file = '../chap06/newsCorpora.csv'\n",
    "data = pd.read_csv(file, encoding='utf-8', header=None, sep='\\t', names=['ID', 'TITLE', 'URL', 'PUBLISHER', 'CATEGORY', 'STORY', 'HOSTNAME', 'TIMESTAMP'])\n",
    "data = data.replace('\"', \"'\")\n",
    "# 特定のpublisherのみ抽出\n",
    "publishers = ['Reuters', 'Huffington Post', 'Businessweek', 'Contactmusic.com', 'Daily Mail']\n",
    "data = data.loc[data['PUBLISHER'].isin(publishers), ['TITLE', 'CATEGORY']].reset_index(drop=True)\n",
    "\n",
    "# 学習用、検証用、評価用に分割する\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, valid_test = train_test_split(data, test_size=0.2, shuffle=True, random_state=64, stratify=data['CATEGORY'])\n",
    "valid, test = train_test_split(valid_test, test_size=0.5, shuffle=True, random_state=64, stratify=valid_test['CATEGORY'])\n",
    "\n",
    "train = train.reset_index(drop=True)\n",
    "valid = valid.reset_index(drop=True)\n",
    "test = test.reset_index(drop=True)\n",
    "\n",
    "# データ数の確認\n",
    "print('学習データ')\n",
    "print(train['CATEGORY'].value_counts())\n",
    "print('検証データ')\n",
    "print(valid['CATEGORY'].value_counts())\n",
    "print('評価データ')\n",
    "print(test['CATEGORY'].value_counts())\n",
    "\n",
    "import re\n",
    "from nltk import stem\n",
    "\n",
    "# データの結合\n",
    "df = pd.concat([train, valid, test], axis=0).reset_index(drop=True)\n",
    "\n",
    "# 前処理\n",
    "def preprocessing(text):\n",
    "    text_clean = re.sub(r'[\\\"\\'.,:;\\(\\)#\\|\\*\\+\\!\\?#$%&/\\]\\[\\{\\}]', '', text)\n",
    "    text_clean = re.sub('[0-9]+', '0', text_clean)\n",
    "    text_clean = re.sub('\\s-\\s', ' ', text_clean)\n",
    "    return text_clean\n",
    "\n",
    "df['TITLE'] = df['TITLE'].apply(preprocessing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10672, 300])\n",
      "tensor([[ 0.0368,  0.0300, -0.0738,  ..., -0.1523,  0.0419, -0.0774],\n",
      "        [ 0.0002, -0.0056, -0.0824,  ..., -0.0544,  0.0776, -0.0214],\n",
      "        [ 0.0266, -0.0166, -0.0877,  ..., -0.0522,  0.0517,  0.0093],\n",
      "        ...,\n",
      "        [-0.0291,  0.0529, -0.1453,  ...,  0.0494,  0.1548, -0.0910],\n",
      "        [-0.0269,  0.1204, -0.0289,  ..., -0.0062,  0.0739, -0.0327],\n",
      "        [ 0.0361,  0.1236,  0.0260,  ..., -0.0099, -0.0193,  0.0262]])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# 平均単語ベクトルの取得\n",
    "def w2v(text):\n",
    "    words = text.rstrip().split()\n",
    "    vec = [model[word] for word in words if word in model]\n",
    "    return np.array(sum(vec) / len(vec))\n",
    "\n",
    "vecs = np.array([])\n",
    "for text in df['TITLE']:\n",
    "    if len(vecs) == 0:\n",
    "        vecs = w2v(text)\n",
    "    else:\n",
    "        vecs = np.vstack([vecs, w2v(text)])\n",
    "\n",
    "# 特徴ベクトルのテンソル化\n",
    "import torch\n",
    "\n",
    "# 乱数のシードを設定\n",
    "torch.manual_seed(1234)\n",
    "np.random.seed(1234)\n",
    "\n",
    "X_train = torch.from_numpy(vecs[:len(train), :])\n",
    "X_valid = torch.from_numpy(vecs[len(train):len(train)+ len(valid), :])\n",
    "X_test = torch.from_numpy(vecs[len(train)+ len(valid):, :])\n",
    "print(X_train.size())\n",
    "print(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ターゲットのテンソル化\n",
    "category_dict = {'b': 0, 't': 1, 'e':2, 'm':3}\n",
    "Y_train = torch.from_numpy(train['CATEGORY'].map(category_dict).values)\n",
    "Y_valid = torch.from_numpy(valid['CATEGORY'].map(category_dict).values)\n",
    "Y_test = torch.from_numpy(test['CATEGORY'].map(category_dict).values)\n",
    "# 保存\n",
    "torch.save(X_train, './X_train.pt')\n",
    "torch.save(X_valid, './X_valid.pt')\n",
    "torch.save(X_test, './X_test.pt')\n",
    "torch.save(Y_train, './y_train.pt')\n",
    "torch.save(Y_valid, './y_valid.pt')\n",
    "torch.save(Y_test, './y_test.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 71. 単層ニューラルネットワークによる予測"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "問題70で保存した行列を読み込み，学習データについて以下の計算を実行せよ．\n",
    "$$\n",
    "    \\hat{ y_1 } = softmax(x_1W), \\\\\n",
    "    \\hat{ Y } = softmax(X_{[1:4]}W)\n",
    "$$\n",
    "ただし，$ softmax $はソフトマックス関数，$ X_{[1:4]} \\in \\mathbb{ R }^{4×d} $は特徴ベクトル$ x_1,x_2,x_3,x_4 $を縦に並べた行列である．\n",
    "$$ \n",
    "    X_{[1:4]}=\n",
    "    \\left(\n",
    "        \\begin{array}{c}\n",
    "            x_1 \\\\\n",
    "            x_2 \\\\\n",
    "            x_3 \\\\\n",
    "            x_4\n",
    "        \\end{array}\n",
    "    \\right)\n",
    "$$\n",
    "行列$ W \\in \\mathbb{ R }^{d\\times L} $は単層ニューラルネットワークの重み行列で，ここではランダムな値で初期化すればよい（問題73以降で学習して求める）．なお，$ \\hat{ y_1 } \\in \\mathbb{ R }^L $は未学習の行列$ W $で事例$ x_1 $を分類したときに，各カテゴリに属する確率を表すベクトルである． 同様に，$ \\hat{ Y } \\in \\mathbb{ R }^{n\\times L} $は，学習データの事例$ x_1,x_2,x_3,x_4 $について，各カテゴリに属する確率を行列として表現している．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLNet(\n",
      "  (fc): Linear(in_features=300, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# モデルの構築\n",
    "from torch import nn\n",
    "\n",
    "class SLNet(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(input_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        logits = self.fc(x)\n",
    "        return logits\n",
    "\n",
    "model = SLNet(300, 4)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0176,  0.0697,  0.0920,  0.0596], grad_fn=<ViewBackward0>)\n",
      "tensor([0.2332, 0.2545, 0.2603, 0.2520], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "logits = model(X_train[0])\n",
    "y_hat_1 = nn.Softmax(dim=-1)(logits)\n",
    "print(logits)\n",
    "print(y_hat_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0176,  0.0697,  0.0920,  0.0596],\n",
      "        [-0.0210,  0.0704,  0.0814,  0.0636],\n",
      "        [ 0.0360,  0.1010,  0.0704,  0.0456],\n",
      "        [ 0.0362,  0.1266,  0.0222,  0.1343]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.2332, 0.2545, 0.2603, 0.2520],\n",
      "        [0.2330, 0.2553, 0.2581, 0.2536],\n",
      "        [0.2432, 0.2595, 0.2517, 0.2455],\n",
      "        [0.2390, 0.2616, 0.2357, 0.2637]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "logits = model(X_train[:4])\n",
    "Y_hat = nn.Softmax(dim=1)(logits)\n",
    "print(logits)\n",
    "print(Y_hat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 72. 損失と勾配の計算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習データの事例$ x1 $と事例集合$ x1,x2,x3,x4 $に対して，クロスエントロピー損失と，行列$ W $に対する勾配を計算せよ．なお，ある事例$ x_i $に対して損失は次式で計算される．\n",
    "$$\n",
    "l_i = −log[事例x_iがy_iに分類される確率]\n",
    "$$\n",
    "ただし，事例集合に対するクロスエントロピー損失は，その集合に含まれる各事例の損失の平均とする．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "損失:  1.3460214138031006\n",
      "勾配: \n",
      "tensor([[ 0.0086,  0.0070, -0.0172,  ..., -0.0355,  0.0098, -0.0181],\n",
      "        [ 0.0094,  0.0076, -0.0188,  ..., -0.0388,  0.0107, -0.0197],\n",
      "        [-0.0273, -0.0222,  0.0546,  ...,  0.1127, -0.0310,  0.0573],\n",
      "        [ 0.0093,  0.0076, -0.0186,  ..., -0.0384,  0.0106, -0.0195]])\n"
     ]
    }
   ],
   "source": [
    "# x_1のロスを求める\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "logits = model(X_train[0])\n",
    "loss = criterion(logits, Y_train[0])\n",
    "print(\"損失: \", loss.item())\n",
    "model.zero_grad()\n",
    "loss.backward()\n",
    "print(\"勾配: \")\n",
    "print(model.fc.weight.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "損失:  1.3807432651519775\n",
      "勾配: \n",
      "tensor([[ 5.0444e-03,  3.9170e-03,  7.1611e-03,  ..., -2.0513e-03,\n",
      "         -4.2177e-03,  5.3509e-04],\n",
      "        [ 3.9538e-05, -6.0439e-03, -1.8717e-02,  ..., -1.5257e-02,\n",
      "         -4.6948e-03, -6.8699e-03],\n",
      "        [-1.0491e-02, -7.2170e-04,  2.5712e-02,  ...,  3.4016e-02,\n",
      "         -7.4130e-03,  1.1578e-02],\n",
      "        [ 5.4069e-03,  2.8486e-03, -1.4156e-02,  ..., -1.6707e-02,\n",
      "          1.6325e-02, -5.2435e-03]])\n"
     ]
    }
   ],
   "source": [
    "# x_1~x_4のロスを求める\n",
    "logits = model(X_train[:4])\n",
    "loss = criterion(logits, Y_train[:4])\n",
    "print(\"損失: \", loss.item())\n",
    "model.zero_grad()\n",
    "loss.backward()\n",
    "print(\"勾配: \")\n",
    "print(model.fc.weight.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 73. 確率的勾配降下法による学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "確率的勾配降下法（SGD: Stochastic Gradient Descent）を用いて，行列$ W $を学習せよ．なお，学習は適当な基準で終了させればよい（例えば「100エポックで終了」など）．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([300])\n",
      "tensor(2)\n",
      "torch.Size([300])\n",
      "tensor(3)\n",
      "torch.Size([300])\n",
      "tensor(2)\n"
     ]
    }
   ],
   "source": [
    "# データセットを作成する\n",
    "import torch.utils.data as data\n",
    "\n",
    "class NewsDataset(data.Dataset):\n",
    "    \"\"\"\n",
    "    newsのDatasetクラス\n",
    "    \n",
    "    Attributes\n",
    "    ----------------------------\n",
    "    X : テンソル\n",
    "        単語ベクトルの平均をまとめたテンソル\n",
    "    y : テンソル\n",
    "        カテゴリをラベル化したテンソル\n",
    "    phase : 'train' or 'val'\n",
    "        学習か訓練かを設定する\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, phase='train'):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.phase = phase\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"全データサイズを返す\"\"\"\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"idxに対応するテンソル形式のデータとラベルを取得\"\"\"\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_dataset = NewsDataset(X_train, Y_train, phase='train')\n",
    "valid_dataset = NewsDataset(X_valid, Y_valid, phase='val')\n",
    "test_dataset = NewsDataset(X_test, Y_test, phase='val')\n",
    "\n",
    "# 動作確認\n",
    "idx = 0\n",
    "print(train_dataset.__getitem__(idx)[0].size())\n",
    "print(train_dataset.__getitem__(idx)[1])\n",
    "print(valid_dataset.__getitem__(idx)[0].size())\n",
    "print(valid_dataset.__getitem__(idx)[1])\n",
    "print(test_dataset.__getitem__(idx)[0].size())\n",
    "print(test_dataset.__getitem__(idx)[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 300])\n",
      "tensor([2])\n"
     ]
    }
   ],
   "source": [
    "# DataLoaderを作成\n",
    "batch_size = 1\n",
    "\n",
    "train_dataloader = data.DataLoader(\n",
    "            train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_dataloader = data.DataLoader(\n",
    "            valid_dataset, batch_size=len(valid_dataset), shuffle=False)\n",
    "test_dataloader = data.DataLoader(\n",
    "            test_dataset, batch_size=len(test_dataset), shuffle=False)\n",
    "\n",
    "dataloaders_dict = {'train': train_dataloader,\n",
    "                    'val': valid_dataloader,\n",
    "                    'test': test_dataloader,\n",
    "                   }\n",
    "\n",
    "# 動作確認\n",
    "batch_iter = iter(dataloaders_dict['train'])\n",
    "inputs, labels = next(batch_iter)\n",
    "print(inputs.size())\n",
    "print(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 10\n",
      "--------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10672/10672 [00:07<00:00, 1473.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.4169, Acc: 0.8569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 61.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.3204, Acc: 0.8936\n",
      "Epoch 2 / 10\n",
      "--------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10672/10672 [00:11<00:00, 925.77it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.3108, Acc: 0.8925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 81.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.2800, Acc: 0.9055\n",
      "Epoch 3 / 10\n",
      "--------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10672/10672 [00:08<00:00, 1248.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.2897, Acc: 0.8996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 43.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.2809, Acc: 0.9063\n",
      "Epoch 4 / 10\n",
      "--------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10672/10672 [00:06<00:00, 1560.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.2794, Acc: 0.9027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 85.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.2728, Acc: 0.9123\n",
      "Epoch 5 / 10\n",
      "--------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10672/10672 [00:06<00:00, 1747.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.2718, Acc: 0.9055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 103.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.2681, Acc: 0.9100\n",
      "Epoch 6 / 10\n",
      "--------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10672/10672 [00:07<00:00, 1461.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.2642, Acc: 0.9096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 64.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.2768, Acc: 0.9078\n",
      "Epoch 7 / 10\n",
      "--------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10672/10672 [00:13<00:00, 767.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.2600, Acc: 0.9117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 36.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.2788, Acc: 0.9055\n",
      "Epoch 8 / 10\n",
      "--------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10672/10672 [00:09<00:00, 1116.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.2576, Acc: 0.9116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 69.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.2700, Acc: 0.9085\n",
      "Epoch 9 / 10\n",
      "--------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10672/10672 [00:07<00:00, 1428.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.2551, Acc: 0.9125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 97.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.2748, Acc: 0.9160\n",
      "Epoch 10 / 10\n",
      "--------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10672/10672 [00:08<00:00, 1189.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.2530, Acc: 0.9136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 60.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.2691, Acc: 0.9115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# 学習\n",
    "\n",
    "# モデルの定義\n",
    "net = SLNet(300, 4)\n",
    "net.train()\n",
    "\n",
    "# 損失関数の定義\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 最適化手法の定義\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# 学習用の関数を定義\n",
    "def train_model(net, dataloaders_dict, criterion, optimizer, num_epochs):\n",
    "    \n",
    "    # epochのループ\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {} / {}'.format(epoch + 1, num_epochs))\n",
    "        print('--------------------------------------------')\n",
    "        \n",
    "        # epochごとの学習と検証のループ\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                net.train() # 訓練モード\n",
    "            else:\n",
    "                net.eval() # 検証モード\n",
    "            \n",
    "            epoch_loss = 0.0 # epochの損失和\n",
    "            epoch_corrects = 0 # epochの正解数\n",
    "            \n",
    "            # データローダーからミニバッチを取り出すループ\n",
    "            for inputs, labels in tqdm(dataloaders_dict[phase]):\n",
    "                optimizer.zero_grad() # optimizerを初期化\n",
    "                \n",
    "                # 順伝播計算(forward)\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = net(inputs)\n",
    "                    loss = criterion(outputs, labels) # 損失を計算\n",
    "                    _, preds = torch.max(outputs, 1) # ラベルを予想\n",
    "                    \n",
    "                    # 訓練時は逆伝播\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                    \n",
    "                    # イテレーション結果の計算\n",
    "                    # lossの合計を更新\n",
    "                    epoch_loss += loss.item() * inputs.size(0)\n",
    "                    # 正解数の合計を更新\n",
    "                    epoch_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "            # epochごとのlossと正解率の表示\n",
    "            epoch_loss = epoch_loss / len(dataloaders_dict[phase].dataset)\n",
    "            epoch_acc = epoch_corrects.double() / len(dataloaders_dict[phase].dataset)\n",
    "            \n",
    "            print('{} Loss: {:.4f}, Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            \n",
    "# 学習を実行する\n",
    "num_epochs = 10\n",
    "train_model(net, dataloaders_dict, criterion, optimizer, num_epochs=num_epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 74. 正解率の計測"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "問題73で求めた行列を用いて学習データおよび評価データの事例を分類したとき，その正解率をそれぞれ求めよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "学習データの正解率: 0.9198\n",
      "検証データの正解率: 0.9115\n",
      "テストデータの正解率: 0.8958\n"
     ]
    }
   ],
   "source": [
    "def calc_acc(net, dataloader):\n",
    "    net.eval()\n",
    "    corrects = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            outputs = net(inputs)\n",
    "            _, preds = torch.max(outputs, 1) # ラベルを予想\n",
    "            corrects += torch.sum(preds == labels.data)\n",
    "    return corrects / len(dataloader.dataset)\n",
    "\n",
    "acc_train = calc_acc(net, train_dataloader)\n",
    "acc_valid = calc_acc(net, valid_dataloader)\n",
    "acc_test = calc_acc(net, test_dataloader)\n",
    "print('学習データの正解率: {:.4f}'.format(acc_train))\n",
    "print('検証データの正解率: {:.4f}'.format(acc_valid))\n",
    "print('テストデータの正解率: {:.4f}'.format(acc_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 75. 損失と正解率のプロットPermalink"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "問題73のコードを改変し，各エポックのパラメータ更新が完了するたびに，訓練データでの損失，正解率，検証データでの損失，正解率をグラフにプロットし，学習の進捗状況を確認できるようにせよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     13\u001b[0m train_corrects \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtrain_loader\u001b[49m:\n\u001b[1;32m     15\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     16\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 各エポックでの訓練データと検証データの損失と正解率を保存するリストを作成\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # 訓練フェーズ\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_corrects = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        train_corrects += torch.sum(preds == labels.data)\n",
    "    train_loss = train_loss / len(train_loader.dataset)\n",
    "    train_acc = train_corrects.double() / len(train_loader.dataset)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "\n",
    "    # 検証フェーズ\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_corrects = 0\n",
    "    for inputs, labels in val_loader:\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        val_loss += loss.item() * inputs.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        val_corrects += torch.sum(preds == labels.data)\n",
    "    val_loss = val_loss / len(val_loader.dataset)\n",
    "    val_acc = val_corrects.double() / len(val_loader.dataset)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_acc)\n",
    "\n",
    "    # 損失と正解率のグラフをプロット\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_accuracies, label='Train Accuracy')\n",
    "    plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 76. チェックポイント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "問題75のコードを改変し，各エポックのパラメータ更新が完了するたびに，チェックポイント（学習途中のパラメータ（重み行列など）の値や最適化アルゴリズムの内部状態）をファイルに書き出せ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 77. ミニバッチ化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "問題76のコードを改変し，$ B $事例ごとに損失・勾配を計算し，行列$ W $の値を更新せよ（ミニバッチ化）．$ B $の値を$ 1,2,4,8,… $と変化させながら，1エポックの学習に要する時間を比較せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# 学習用の関数を定義\n",
    "def train_model(net, dataloaders_dict, criterion, optimizer, num_epochs):\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    valid_loss = []\n",
    "    valid_acc = []\n",
    "    # epochのループ\n",
    "    for epoch in range(num_epochs):\n",
    "        # 開始時刻の記録\n",
    "        start = time.time()\n",
    "        print('Epoch {} / {}'.format(epoch + 1, num_epochs))\n",
    "        print('--------------------------------------------')\n",
    "        \n",
    "        # epochごとの学習と検証のループ\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                net.train() # 訓練モード\n",
    "            else:\n",
    "                net.eval() # 検証モード\n",
    "            \n",
    "            epoch_loss = 0.0 # epochの損失和\n",
    "            epoch_corrects = 0 # epochの正解数\n",
    "            \n",
    "            # データローダーからミニバッチを取り出すループ\n",
    "            for inputs, labels in tqdm(dataloaders_dict[phase]):\n",
    "                optimizer.zero_grad() # optimizerを初期化\n",
    "                \n",
    "                # 順伝播計算(forward)\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = net(inputs)\n",
    "                    loss = criterion(outputs, labels) # 損失を計算\n",
    "                    _, preds = torch.max(outputs, 1) # ラベルを予想\n",
    "                    \n",
    "                    # 訓練時は逆伝播\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                    \n",
    "                    # イテレーション結果の計算\n",
    "                    # lossの合計を更新\n",
    "                    epoch_loss += loss.item() * inputs.size(0)\n",
    "                    # 正解数の合計を更新\n",
    "                    epoch_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "            # epochごとのlossと正解率の表示\n",
    "            epoch_loss = epoch_loss / len(dataloaders_dict[phase].dataset)\n",
    "            epoch_acc = epoch_corrects.double() / len(dataloaders_dict[phase].dataset)\n",
    "            if phase == 'train':\n",
    "                train_loss.append(epoch_loss)\n",
    "                train_acc.append(epoch_acc)\n",
    "            else:\n",
    "                valid_loss.append(epoch_loss)\n",
    "                valid_acc.append(epoch_acc)\n",
    "            \n",
    "            print('{} Loss: {:.4f}, Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "        # 修了時刻の記録\n",
    "        end = time.time()\n",
    "        calc_time = end - start\n",
    "        print('batch_size {} calc_time: {:.4f} sec'.format(batch_size, calc_time))\n",
    "    return train_loss, train_acc, valid_loss, valid_acc, calc_time\n",
    "\n",
    "\n",
    "# 学習を実行する\n",
    "batch_sizes = [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048]\n",
    "cpu_times = []\n",
    "for batch_size in batch_sizes:\n",
    "    print('batch_size: {}'.format(batch_size))\n",
    "    # DataLoaderを作成\n",
    "    train_dataloader = data.DataLoader(\n",
    "                train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    valid_dataloader = data.DataLoader(\n",
    "                valid_dataset, batch_size=len(valid_dataset), shuffle=False)\n",
    "    test_dataloader = data.DataLoader(\n",
    "                test_dataset, batch_size=len(test_dataset), shuffle=False)\n",
    "\n",
    "    dataloaders_dict = {'train': train_dataloader,\n",
    "                        'val': valid_dataloader,\n",
    "                        'test': test_dataloader,\n",
    "                       }\n",
    "    # モデルの定義\n",
    "    net = SLNet(300, 4)\n",
    "    net.train()\n",
    "\n",
    "    # 損失関数の定義\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # 最適化手法の定義\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "    num_epochs = 1\n",
    "    train_loss, train_acc, valid_loss, valid_acc, calc_time = \\\n",
    "                        train_model(net, dataloaders_dict, criterion, optimizer,\n",
    "                                    num_epochs=num_epochs)\n",
    "    cpu_times.append(calc_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 78. GPU上での学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "問題77のコードを改変し，GPU上で学習を実行せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 79. 多層ニューラルネットワーク"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "問題78のコードを改変し，バイアス項の導入や多層化など，ニューラルネットワークの形状を変更しながら，高性能なカテゴリ分類器を構築せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
